{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 PyTorch基础：Tensor和Autograd\n",
    "\n",
    "## 3.1 Tensor\n",
    "\n",
    "Tensor，又名张量，读者可能对这个名词似曾相识，因它不仅在PyTorch中出现过，它也是Theano、TensorFlow、\n",
    "Torch和MxNet中重要的数据结构。关于张量的本质不乏深度的剖析，但从工程角度来讲，可简单地认为它就是一个数组，且支持高效的科学计算。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）和更高维的数组（高阶数据）。Tensor和Numpy的ndarrays类似，但PyTorch的tensor支持GPU加速。\n",
    "\n",
    "本节将系统讲解tensor的使用，力求面面俱到，但不会涉及每个函数。对于更多函数及其用法，读者可通过在IPython/Notebook中使用函数名加`?`查看帮助文档，或查阅PyTorch官方文档[^1]。\n",
    "\n",
    "[^1]: http://docs.pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin\n",
    "from __future__ import print_function\n",
    "import torch  as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1.1 基础操作\n",
    "\n",
    "学习过Numpy的读者会对本节内容感到非常熟悉，因tensor的接口有意设计成与Numpy类似，以方便用户使用。但不熟悉Numpy也没关系，本节内容并不要求先掌握Numpy。\n",
    "\n",
    "从接口的角度来讲，对tensor的操作可分为两类：\n",
    "\n",
    "1. `torch.function`，如`torch.save`等。\n",
    "2. 另一类是`tensor.function`，如`tensor.view`等。\n",
    "\n",
    "为方便使用，对tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如`torch.sum (torch.sum(a, b))`与`tensor.sum (a.sum(b))`功能等价。\n",
    "\n",
    "而从存储的角度来讲，对tensor的操作又可分为两类：\n",
    "\n",
    "1. 不会修改自身的数据，如 `a.add(b)`， 加法的结果会返回一个新的tensor。\n",
    "2. 会修改自身的数据，如 `a.add_(b)`， 加法的结果仍存储在a中，a被修改了。\n",
    "\n",
    "函数名以`_`结尾的都是inplace方式, 即会修改调用者自己的数据，在实际应用中需加以区分。\n",
    "\n",
    "#### 创建Tensor\n",
    "\n",
    "在PyTorch中新建tensor的方法有很多，具体如表3-1所示。\n",
    "\n",
    "表3-1: 常见新建tensor的方法\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(\\*sizes)|全1Tensor|\n",
    "|zeros(\\*sizes)|全0Tensor|\n",
    "|eye(\\*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step)|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(\\*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu).\n",
    "\n",
    "\n",
    "其中使用`Tensor`函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 3.6893e+19, 0.0000e+00],\n",
       "        [3.6893e+19, 1.1303e+26, 4.5778e-41]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定tensor的形状\n",
    "a = t.Tensor(2, 3)\n",
    "a # 数值取决于内存空间的状态，print时候可能overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用list的数据创建tensor\n",
    "b = t.Tensor([[1,2,3],[4,5,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist() # 把tensor转为list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.size()`返回`torch.Size`对象，它是tuple的子类，但其使用方式与tuple略有区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Size'>\n",
      "torch.Size([2, 3])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "print(type(b_size)) # <class 'torch.Size'>\n",
    "print(b_size) \n",
    "print(isinstance(b_size,tuple)) # 是tuple的子类吗？  True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(b.numel()) # b中元素总个数，2*3，等价于b.nelement()\n",
    "print(b.nelement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 1.8754e+28],\n",
      "        [4.0611e-08, 1.0780e-08, 1.0358e-11]]) tensor([2., 3.])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "下面这个代码的意思是b_size  和 (2,3) 这个效果是不一样的\n",
    "\"\"\"\n",
    "# 创建一个和b形状一样的tensor\n",
    "c = t.Tensor(b_size)\n",
    "# 创建一个元素为2和3的tensor\n",
    "d = t.Tensor((2, 3))\n",
    "print(c, d)\n",
    "\n",
    "print(b.shape)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`tensor.size()`，还可以利用`tensor.shape`直接查看tensor的形状，`tensor.shape`等价于`tensor.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，`t.Tensor(*sizes)`创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。其它常用的创建tensor的方法举例如下。\n",
    "\n",
    "【lawson:这个原因是不是因为python是解释型语言？】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)  # 从1到6，步长为2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  5.5000, 10.0000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1163, -0.1111, -0.0152],\n",
       "        [-0.3127,  2.1007,  0.1645]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3, device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 1, 2, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 长度为5的随机排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3, dtype=t.int) # 对角线为1, 不要求行列数一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tensor`是在0.4版本新增加的一个新版本的创建tensor方法，使用的方法，和参数几乎和`np.array`完全一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar: tensor(3.1416), shape of sclar: torch.Size([])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new(): data must be a sequence (got float)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-97c695dcfd39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.14159\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scalar: %s, shape of sclar: %s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): data must be a sequence (got float)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "注意t.tensor()  和 t.Tensor()的区别\n",
    "t.tensor()中需传入的是具体的数据；\n",
    "t.Tensor()中需传入的是tensor 的size\n",
    "\"\"\"\n",
    "scalar = t.tensor(3.14159)   \n",
    "print('scalar: %s, shape of sclar: %s' %(scalar, scalar.shape))\n",
    "print(sca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector = t.tensor([1, 2])\n",
    "print(vector)\n",
    "print(vector.shape)\n",
    "#print('vector: %s, shape of vector: %s' %(vector, vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector2 = t.tensor((1, 2))  # 无论t.tensor()中使用的是什么括号，得到的结果都是一样的\n",
    "print(vector2)\n",
    "print(vector2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 3.6893e+19]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = t.Tensor(1,2) # 注意和t.tensor([1, 2])的区别\n",
    "print(tensor)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1000, 1.2000],\n",
       "         [2.2000, 3.1000],\n",
       "         [4.9000, 5.2000]]),\n",
       " torch.Size([3, 2]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = t.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "matrix,matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "a=t.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                     dtype=t.float64,\n",
    "                     device=t.device('cpu')) \n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_tensor = t.tensor([])\n",
    "empty_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常用Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`tensor.view`方法可以调整tensor的形状，但必须保证调整前后元素总数一致。`view`不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候`squeeze`和`unsqueeze`两个函数就派上用场了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "b = a.view(-1, 3) # 当某一维为-1的时候，会自动计算它的大小\n",
    "b.shape\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1) # 注意形状，在第1维（下标从0开始）上增加“１”  \n",
    "print(b)\n",
    "#等价于 b[:,None]\n",
    "b[:, None].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2]],\n",
       "\n",
       "        [[3, 4, 5]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(-2) # -2表示倒数第二个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0, 1, 2],\n",
       "          [3, 4, 5]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c.squeeze(0) # 压缩第0维的“１”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.squeeze() # 把所有维度为“1”的压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b # a修改，b作为view之后的，也会跟着修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`resize`是另一种可用来调整`size`的方法，但与`view`不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5],\n",
       "        [  0,   0, 112]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3, 3) # 旧的数据依旧保存着，多出的大小会分配新空间\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引操作\n",
    "\n",
    "Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1741,  1.4335, -0.8156,  0.7622],\n",
       "        [-0.6334, -1.4628, -0.7428,  0.0410],\n",
       "        [-0.6551,  1.0258,  2.0572,  0.3923]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1741,  1.4335, -0.8156,  0.7622])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] # 第0行(下标从0开始)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1741, -0.6334, -0.6551])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0] # 第0列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.8156)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2] # 第0行第2个元素，等价于a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7622)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, -1] # 第0行最后一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1741,  1.4335, -0.8156,  0.7622],\n",
       "        [-0.6334, -1.4628, -0.7428,  0.0410]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2] # 前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1741,  1.4335],\n",
       "        [-0.6334, -1.4628]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2, 0:2] # 前两行，第0,1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1741, 1.4335]])\n",
      "tensor([1.1741, 1.4335])\n"
     ]
    }
   ],
   "source": [
    "print(a[0:1, :2]) # 第0行，前两列 \n",
    "print(a[0, :2]) # 注意两者的区别：形状不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# None类似于np.newaxis, 为a新增了一个轴\n",
    "# 等价于a.view(1, a.shape[0], a.shape[1])\n",
    "a[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None].shape # 等价于a[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4, 1, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 1, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1 # 返回一个ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1741, 1.4335, 1.0258, 2.0572])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>1] # 等价于a.masked_select(a>1)\n",
    "# 选择结果与原tensor不共享内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1741,  1.4335, -0.8156,  0.7622],\n",
       "        [-0.6334, -1.4628, -0.7428,  0.0410]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0,1])] # 第0行和第1行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其它常用的选择函数如表3-2所示。\n",
    "\n",
    "表3-2常用的选择函数\n",
    "\n",
    "函数|功能|\n",
    ":---:|:---:|\n",
    "index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "non_zero(input)|非0元素的下标\n",
    "gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样\n",
    "\n",
    "\n",
    "`gather`是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "三维tensor的`gather`操作同理，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  5, 10, 15]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3],\n",
       "        [ 6],\n",
       "        [ 9],\n",
       "        [12]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.LongTensor([[3,2,1,0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12,  9,  6,  3]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素，注意与上面的不同\n",
    "index = t.LongTensor([[3,2,1,0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  3],\n",
       "        [ 5,  6],\n",
       "        [10,  9],\n",
       "        [15, 12]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与`gather`相对应的逆操作是`scatter_`，`gather`把数据从input中按index取出，而`scatter_`是把取出的数据再放回去。注意`scatter_`函数是inplace操作。\n",
    "\n",
    "```python\n",
    "out = input.gather(dim, index)\n",
    "-->近似逆操作\n",
    "out = Tensor()\n",
    "out.scatter_(dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-328c60aeafe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 把两个对角线元素放回去到指定位置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# 把两个对角线元素放回去到指定位置\n",
    "c = t.zeros(4,4)\n",
    "c.scatter_(1, index, b.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用`tensor.item()`, 这个方法只对包含一个元素的tensor适用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0] #依旧是tensor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0].item() # python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a[0:1, 0:1, None]\n",
    "print(d.shape)\n",
    "d.item() # 只包含一个元素的tensor即可调用tensor.item,与形状无关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[0].item()  ->\n",
    "# raise ValueError: only one element tensors can be converted to Python scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级索引\n",
    "PyTorch在0.2版本中完善了索引操作，目前已经支持绝大多数numpy的高级索引[^10]。高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的Tensor共享内存。 \n",
    "[^10]: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,27).view(3,3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 24])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]] # x[1,1,2]和x[2,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 10,  1])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]] # x[2,0,1],x[1,0,1],x[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], ...] # x[0] 和 x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor类型\n",
    "\n",
    "Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过`t.set_default_tensor_type` 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有`1000*1000*1000=10^9`个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。\n",
    "\n",
    "[^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n",
    "\n",
    "表3-3: tensor数据类型\n",
    "\n",
    "| Data type                | dtype                             | CPU tensor                                                   | GPU tensor                |\n",
    "| ------------------------ | --------------------------------- | ------------------------------------------------------------ | ------------------------- |\n",
    "| 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`                                          | `torch.cuda.FloatTensor`  |\n",
    "| 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor`                                         | `torch.cuda.DoubleTensor` |\n",
    "| 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`                                           | `torch.cuda.HalfTensor`   |\n",
    "| 8-bit integer (unsigned) | `torch.uint8`                     | [`torch.ByteTensor`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor) | `torch.cuda.ByteTensor`   |\n",
    "| 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`                                           | `torch.cuda.CharTensor`   |\n",
    "| 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`                                          | `torch.cuda.ShortTensor`  |\n",
    "| 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`                                            | `torch.cuda.IntTensor`    |\n",
    "| 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`                                           | `torch.cuda.LongTensor`   |\n",
    "\n",
    " \n",
    "\n",
    "各数据类型之间可以互相转换，`type(new_type)`是通用的做法，同时还有`float`、`long`、`half`等快捷方法。CPU tensor与GPU tensor之间的互相转换通过`tensor.cuda`和`tensor.cpu`方法实现，此外还可以使用`tensor.to(device)`。Tensor还有一个`new`方法，用法与`t.Tensor`一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。`torch.*_like(tensora)` 可以生成和`tensora`拥有同样属性(类型，形状，cpu/gpu)的新tensor。 `tensor.new_*(new_shape)` 新建一个不同形状的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置默认tensor，注意参数是字符串\n",
    "t.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2,3)\n",
    "a.dtype # 现在a是DoubleTensor,dtype是float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复之前的默认设置\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把a转成FloatTensor，等价于b=a.type(t.FloatTensor)\n",
    "b = a.float() \n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.2148e-313, 1.1459e-312, 1.2095e-312],\n",
       "        [1.0610e-312, 1.4217e-312, 1.4005e-312]], dtype=torch.float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new(2,3) # 等价于torch.DoubleTensor(2,3)，建议使用a.new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a) #等价于t.zeros(a.shape,dtype=a.dtype,device=a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]], dtype=torch.int16)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a, dtype=t.int16) #可以修改某些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3111, 0.3843, 0.1729],\n",
       "        [0.2693, 0.6378, 0.9917]], dtype=torch.float64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_ones(4,5, dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_tensor([3,4]) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逐元素操作\n",
    "\n",
    "这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。\n",
    "\n",
    "表3-4: 常见的逐元素操作\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|\n",
    "|cos/sin/asin/atan2/cosh..|相关三角函数|\n",
    "|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|\n",
    "|clamp(input, min, max)|超过min和max部分截断|\n",
    "|sigmod/tanh..|激活函数\n",
    "\n",
    "对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如`a ** 2` 等价于`torch.pow(a,2)`, `a * 2`等价于`torch.mul(a,2)`。\n",
    "\n",
    "其中`clamp(x, min, max)`的输出满足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min,  & \\text{if  } x_i \\lt min \\\\\n",
    "x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n",
    "max,  & \\text{if  } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5403, -0.4161],\n",
       "        [-0.9900, -0.6536,  0.2837]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [0., 1., 2.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3 # 等价于t.fmod(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  4.],\n",
       "        [ 9., 16., 25.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2 # 等价于t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 4., 5.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.8415,  0.9093],\n",
       "        [ 0.1411, -0.7568, -0.9589]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.sin_() # 效果同 a = a.sin();b=a ,但是更高效节省显存\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  归并操作 \n",
    "此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法`sum`，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用归并操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位数/众数|\n",
    "|norm/dist|范数/距离|\n",
    "|std/var|标准差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多数函数都有一个参数**`dim`**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：\n",
    "\n",
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取决于参数`keepdim`，`keepdim=True`会保留维度`1`。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "b.sum(dim = 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  3],\n",
       "        [ 3,  7, 12]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "a.cumsum(dim=1) # 沿着行累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较\n",
    "比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。\n",
    "\n",
    "表3-6: 常用比较函数\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|\n",
    "|topk|最大的k个数|\n",
    "|sort|排序|\n",
    "|max/min|比较两个tensor最大最小值|\n",
    "\n",
    "表中第一行的比较操作已经实现了运算符重载，因此可以使用`a>=b`、`a>b`、`a!=b`、`a==b`，其返回结果是一个`ByteTensor`，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：\n",
    "- t.max(tensor)：返回tensor中最大的一个数\n",
    "- t.max(tensor,dim)：指定维上最大的数，返回tensor和下标\n",
    "- t.max(tensor1, tensor2): 比较两个tensor相比较大的元素\n",
    "\n",
    "至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  3.,  6.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 6.,  3.,  0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a>b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 12., 15.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>b] # a中大于b的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(values=tensor([15.,  6.]), indices=tensor([0, 0]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(b, dim=1) \n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# 第二个返回值的0和0表示上述最大的数是该行第0个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15., 12.,  9.],\n",
       "        [ 9., 12., 15.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10., 10.],\n",
       "        [10., 12., 15.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较a和10较大的元素\n",
    "t.clamp(a, min=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 线性代数\n",
    "\n",
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。\n",
    "\n",
    "表3-7: 常用的线性代数函数\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/badbmm..|矩阵运算\n",
    "|t|转置|\n",
    "|dot/cross|内积/外积\n",
    "|inverse|求逆矩阵\n",
    "|svd|奇异值分解\n",
    "\n",
    "具体使用说明请参见官方文档[^3]，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的`.contiguous`方法将其转为连续。\n",
    "[^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.t()\n",
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  9.],\n",
       "        [ 3., 12.],\n",
       "        [ 6., 15.]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Tensor和Numpy\n",
    "\n",
    "Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2, 3],dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 也可以直接将numpy对象传入Tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]=100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy() # a, b, c三个对象共享内存\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "# 注意和上面的a的区别（dtype不是float32）\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 此处进行拷贝，不共享内存\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.from_numpy(a) # 注意c的类型（DoubleTensor）\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b # b与a不共享内存，所以即使a改变了，b也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1., 100.,   1.],\n",
       "        [  1.,   1.,   1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c # c与a共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：** 不论输入的类型是什么，t.tensor都会进行数据拷贝，不会共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = t.tensor(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 100.,   1.],\n",
       "       [  1.,   1.,   1.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0,0]=0\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。\n",
    "Numpy的广播法则定义如下：\n",
    "\n",
    "- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐\n",
    "- 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 \n",
    "- 当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状\n",
    "\n",
    "PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：\n",
    "\n",
    "- `unsqueeze`或者`view`，或者tensor[None],：为数据某一维的形状补1，实现法则1\n",
    "- `expand`或者`expand_as`，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。\n",
    "\n",
    "注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动广播法则\n",
    "# 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，\n",
    "#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,\n",
    "# 第二步: a和b在第一维和第三维形状不一样，其中一个为1 ，\n",
    "#               可以利用广播法则扩展，两个形状都变成了（2，3，2）\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动广播法则\n",
    "# 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\n",
    "a[None].expand(2, 3, 2) + b.expand(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存\n",
    "e = a.unsqueeze(0).expand(10000000000000, 3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 内部结构\n",
    "\n",
    "tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。\n",
    "\n",
    "一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。\n",
    "\n",
    "![图3-1: Tensor的数据结构](imgs/tensor_data_structure.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.LongStorage of size 6]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-402b46e407dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "print(a.storage())\n",
    "#print(a.head())  怎么打印出tensor的头部信息？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个对象的id值可以看作它在内存中的地址\n",
    "# storage的内存地址一样，即是同一个storage\n",
    "id(b.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 100,   2],\n",
       "        [  3,   4,   5]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a改变，b也随之改变，因为他们共享storage\n",
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 100\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       "[torch.LongStorage of size 6]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:] \n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61277776, 61277760)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr(), a.data_ptr() # data_ptr返回tensor首元素的内存地址\n",
    "# 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  100, -100,    3,    4,    5])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100 # c[0]的内存地址对应a[2]的内存地址\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6666,  100, -100],\n",
       "        [   3,    4,    5]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = t.LongTensor(c.storage())\n",
    "d[0] = 6666\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面４个tensor共享storage\n",
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2, 0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), c.storage_offset(), d.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2] # 隔2行/列取一个元素\n",
    "id(e.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 1), (6, 2))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), e.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。\n",
    "此外有些操作会导致tensor不连续，这时需调用`tensor.contiguous`方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。\n",
    "另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 其它有关Tensor的话题\n",
    "这部分的内容不好专门划分一小节，但是笔者认为仍值得读者注意，故而将其放在这一小节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU/CPU\n",
    "tensor可以很随意的在gpu/cpu上传输。使用`tensor.cuda(device_id)`或者`tensor.cpu()`。另外一个更通用的方法是`tensor.to(device)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = t.randn(3,4, device=t.device('cuda:1'))\n",
    "    # 等价于\n",
    "    # a.t.randn(3,4).cuda(1)\n",
    "    # 但是前者更快\n",
    "    a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8934,  0.8776,  2.3714, -0.0861],\n",
       "        [-0.2218,  1.7379,  0.5166,  0.2940],\n",
       "        [ 1.1621,  0.6702, -0.4791, -0.7298]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device('cpu')\n",
    "a.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**\n",
    "- 尽量使用`tensor.to(device)`, 将`device`设为一个可配置的参数，这样可以很轻松的使程序同时兼容GPU和CPU\n",
    "- 数据在GPU之中传输的速度要远快于内存(CPU)到显存(GPU), 所以尽量避免频繁的在内存和显存中传输数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 持久化\n",
    "Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的`pickle`模块，在load时还可将GPU tensor映射到CPU或其它GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = a.cuda(1) # 把a转为GPU1上的tensor,\n",
    "    t.save(a,'a.pth')\n",
    "\n",
    "    # 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)\n",
    "    b = t.load('a.pth')\n",
    "    # 加载为c, 存储于CPU\n",
    "    c = t.load('a.pth', map_location=lambda storage, loc: storage)\n",
    "    # 加载为d, 存储于GPU0上\n",
    "    d = t.load('a.pth', map_location={'cuda:1':'cuda:0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是`for`循环。在科学计算程序中应当极力避免使用Python原生的`for循环`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i,j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "    return t.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 997 µs per loop\n",
      "The slowest run took 5.80 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10 loops, best of 3: 4.91 µs per loop\n"
     ]
    }
   ],
   "source": [
    "x = t.zeros(100)\n",
    "y = t.ones(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见二者有超过几十倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯，千万避免对较大的tensor进行逐元素遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外还有以下几点需要注意：\n",
    "- 大多数`t.function`都有一个参数`out`，这时候产生的结果将保存在out指定tensor之中。\n",
    "- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。\n",
    "- `t.set_printoptions`可以用来设置打印tensor时的数值精度和格式。\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19999999) tensor(19999998)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(19999999), tensor(19999998))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 20000000)\n",
    "print(a[-1], a[-2]) # 32bit的IntTensor精度有限导致溢出\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 20000000, out=b) # 64bit的LongTensor不会溢出\n",
    "b[-1],b[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2645,  0.0270, -0.7174],\n",
       "        [ 0.4271,  0.0869,  0.6367]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2645452023,  0.0270472951, -0.7174307108],\n",
       "        [ 0.4270690084,  0.0868919790,  0.6366676688]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=10)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 小试牛刀：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n",
    "$$\n",
    "然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "#%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "device = t.device('cpu') #如果你想用gpu，改成t.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子，保证在不同电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000) \n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' 产生随机数据：y=x*2+3，加上了一些噪声  =》 向量的数乘运算'''  \n",
    "    x = t.rand(batch_size, 1, device=device) * 5 # *5表示的是值扩大5倍        \n",
    "    y = x * 2 + 3 +  t.randn(batch_size, 1, device=device) # \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "tensor([[3.3075],\n",
      "        [1.3796],\n",
      "        [0.0356],\n",
      "        [4.5593]]) tensor([[ 9.8329],\n",
      "        [ 5.2154],\n",
      "        [ 3.1371],\n",
      "        [10.6250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9c70533910>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 来看看产生的x-y分布\n",
    "x, y = get_fake_data(batch_size=4)\n",
    "plt.scatter(x.squeeze().cpu().numpy(), y.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "w:  1.9551701545715332 b:  2.8530826568603516\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1, 1).to(device)\n",
    "b = t.zeros(1, 1).to(device)\n",
    "\"\"\"\n",
    "上面这个操作得到的w 和 b的值分别如下：\n",
    "tensor([[0.6711]]) tensor([[0.]])\n",
    "\"\"\"\n",
    "\n",
    "lr =0.02 # 学习率\n",
    "\n",
    "for ii in range(500):\n",
    "    x, y = get_fake_data(batch_size=4) \n",
    "    \n",
    "    #step1. forward：计算loss\n",
    "    \"\"\"\n",
    "    mm做的是一个矩阵乘法的操作。 x.mm(w)相当于torch.(x,w)这个操作\n",
    "    x.size() = [4,1]，乘以w得到的仍然是[4,1]的列向量\n",
    "    \n",
    "    b.expand_as(y)  这个操作的意思是用于将tensor b转换成y 这种tensor        \n",
    "    \"\"\"\n",
    "    y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);  for python3 only\n",
    "    loss = 0.5 * (y_pred - y) ** 2 # 均方误差\n",
    "    \n",
    "    loss = loss.mean() # 返回这个tensor 各个dim和的均值\n",
    "    \n",
    "    #step2. backward：手动计算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    \n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "    \n",
    "    if ii%50 ==0:\n",
    "       \n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 6).view(-1, 1)\n",
    "        y = x.float().mm(w) + b.expand_as(x)\n",
    "        plt.plot(x.cpu().numpy(), y.cpu().numpy()) # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=32) \n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "        \n",
    "        plt.xlim(0, 5)\n",
    "        plt.ylim(0, 13)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "print('w: ', w.item(), 'b: ', b.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3719],\n",
      "        [0.4788]])\n",
      "tensor([[0.7438],\n",
      "        [0.9576]])\n"
     ]
    }
   ],
   "source": [
    "x=t.rand(2,1)\n",
    "print(x)\n",
    "\n",
    "y = x* 2 # 注意这里的乘积方式，相当于数乘运算\n",
    "print(y) # tensor 可以"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见程序已经基本学出w=2、b=3，并且图中直线和数据已经实现较好的拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面提到了许多操作，但是只要掌握了这个例子基本上就可以了，其他的知识，读者日后遇到的时候，可以再看看这部份的内容或者查找对应文档。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
